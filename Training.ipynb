{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687149c7-ccd6-4fd0-8d88-b85a57f36d51",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b06d3d-379f-46c1-8c09-f5ce88d60fcb",
   "metadata": {},
   "source": [
    "- https://jaketae.github.io/study/word2vec/\n",
    "- https://www.geeksforgeeks.org/word-embeddings-in-nlp/\n",
    "- https://www.youtube.com/playlist?list=PLhWB2ZsrULv-wEM8JDKA1zk8_2Lc88I-s\n",
    "- https://towardsdatascience.com/skip-gram-neural-network-from-scratch-485f2e688238\n",
    "- https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "- https://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8558ba9b-809b-4c90-890b-0a64a3232efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0017c-1fce-481a-83fe-5b80001bc50d",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a867622f-a851-437b-a90a-bc6ddddcb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a string into a list of words\n",
    "def tokenize(document):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(document.lower())\n",
    "\n",
    "# Tokenizes each string in the list.\n",
    "# by_sentence splits each document into sentences beforehand\n",
    "def batch_tokenize(documents, by_sentence):\n",
    "    if by_sentence:\n",
    "        return list(itertools.chain.from_iterable(\n",
    "            [[tokenize(sentence) for sentence in split_into_sentences(document)] for document in documents]\n",
    "        ))\n",
    "    else:\n",
    "        return [tokenize(document) for document in documents]\n",
    "\n",
    "# From a list of tokenized documents gets unique vocabulary words with their frequencies\n",
    "def generate_vocab_counts(documents):\n",
    "    vocab = defaultdict(int)\n",
    "    for document in documents:\n",
    "         for word in document:\n",
    "             vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae2475e1-0801-4170-8a31-b37620b3034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co|Bros)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "multiple_dots = r'\\.{2,}'\n",
    "\n",
    "# Split a string into sentences\n",
    "# https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the text into sentences.\n",
    "\n",
    "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead \n",
    "    to incorrect splitting because they are used as markers for splitting.\n",
    "\n",
    "    :param text: text to be split into sentences\n",
    "    :type text: str\n",
    "\n",
    "    :return: list of sentences\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83e14749-466a-47d7-963e-c87ae753be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all text files and tokenize\n",
    "mario_texts = []\n",
    "data_dir = 'data/'\n",
    "# for file_name in os.listdir(data_dir):\n",
    "#     if not file_name.endswith(\".txt\"):\n",
    "#         continue\n",
    "#     with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "#         mario_texts.append(file.read())\n",
    "\n",
    "with open(os.path.join(data_dir, \"Mario.txt\"), 'r', encoding='utf-8') as file:\n",
    "    mario_texts.append(file.read())\n",
    "\n",
    "with open(os.path.join(data_dir, \"Boo.txt\"), 'r', encoding='utf-8') as file:\n",
    "    mario_texts.append(file.read())\n",
    "\n",
    "mario_documents = batch_tokenize(mario_texts, False)\n",
    "mario_documents_sentences = batch_tokenize(mario_texts, True)\n",
    "mario_vocab_counts = generate_vocab_counts(mario_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacae55-d443-49e7-8ce4-54de56db5b5b",
   "metadata": {},
   "source": [
    "## Non-Neural Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c3f7f18-c709-43e4-a74f-34ac0984a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a one-hot representation of a word in a vocab list\n",
    "def one_hot_str(word, vocab_to_id):\n",
    "    return one_hot_int(vocab_to_id[word], len(vocab_to_id))\n",
    "\n",
    "def one_hot_int(index, vocab_size):\n",
    "    out = np.zeros(vocab_size)\n",
    "    out[index] = 1\n",
    "    return out\n",
    "\n",
    "def batch_one_hot_str(words, vocab_to_id):\n",
    "    return [one_hot_int(word, vocab_to_id) for word in words]\n",
    "\n",
    "def batch_one_hot_int(indices, vocab_size):\n",
    "    return [one_hot_int(index, vocab_size) for index in indices if index >= 0]\n",
    "\n",
    "def batch_one_hot(words, vocab, index=False):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83ae9f76-648d-4b3f-9ebb-c2338ec16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document and frequency based vectorization\n",
    "def tfidf(vocab_to_id, documents):\n",
    "    tf = []\n",
    "    idf = np.zeros(len(vocab_to_id))\n",
    "    for document in documents:\n",
    "        tf_row = np.zeros(len(vocab_to_id))\n",
    "        for word in document:\n",
    "            tf_row[vocab_to_id[word]] += 1\n",
    "        tf_row /= len(document)\n",
    "        tf.append(tf_row)\n",
    "        idf += np.where(tf_row > 0, 1, 0)\n",
    "    idf = np.log(len(documents) / idf)\n",
    "    return tf * idf     \n",
    "\n",
    "mario_tfidf = tfidf(mario_vocab_to_id, mario_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e865e-5706-437d-a171-425b657cf6a1",
   "metadata": {},
   "source": [
    "## Skipgram and CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6a3809c-4df1-4ee2-9f35-82e655a73223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of sampling a word given its frequency\n",
    "def subsample_prob(frequency, subsample_rate):\n",
    "    return (math.sqrt(frequency / subsample_rate) + 1) * (subsample_rate / frequency)\n",
    "\n",
    "def subsample(vocab_counts, documents, subsample_rate):\n",
    "    total_count = sum(vocab_counts.values())\n",
    "    vocab_subsample_rate = {key: subsample_prob(vocab_counts[key] / total_counts, subsample_rate) for key in vocab_counts.keys()}\n",
    "    np.random.seed(seed)\n",
    "    documents_copy = documents.copy()\n",
    "    for document in documents_copy:\n",
    "        i = 0\n",
    "        while i < len(document):\n",
    "            if np.random.uniform(0, 1) < vocab_subsample_rate[document[i]]:\n",
    "                document.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "    return documents_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55747b8d-e4bd-4727-b59a-971e9f96afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns indices in the vocab\n",
    "def skipgram(documents, vocab_counts, vocab_to_id, left_context_window, right_context_window, k_negsample):\n",
    "    vocab_negsample_rate = {key: vocab_counts[key]**(3/4) for key in vocab_counts.keys()}\n",
    "    negsample_rate_sum = sum(vocab_negsample_rate.values())\n",
    "    vocab_negsample_rate = {key: vocab_negsample_rate[key] / negsample_rate_sum for key in vocab_counts.keys()}\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        for i in range(len(document)):\n",
    "            target = document[i]\n",
    "            target_index = vocab_to_id[target]\n",
    "\n",
    "            for j in range(max(0, i - left_context_window), min(i + right_context_window + 1, len(document))):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                targets.append(target_index)\n",
    "                contexts.append(vocab_to_id[context])\n",
    "                labels.append(1)\n",
    "\n",
    "                for neg_context_index in np.random.choice(range(len(vocab_negsample_rate)), \n",
    "                                                         size=k_negsample, replace=False, \n",
    "                                                         p=list(vocab_negsample_rate.values())):\n",
    "                    targets.append(target_index)\n",
    "                    contexts.append(neg_context_index)\n",
    "                    labels.append(0)\n",
    "                    \n",
    "    return (jnp.array(contexts), jnp.array(targets), jnp.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ec6fbd9-359f-44a3-bc43-114056de4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context filled with -1 to reach homgenous shape\n",
    "def cbow(documents, vocab_counts, left_context_window, right_context_window, k_negsample):\n",
    "    vocab_negsample_rate = {key: vocab_counts[key]**(3/4) for key in vocab_counts.keys()}\n",
    "    negsample_rate_sum = sum(vocab_negsample_rate.values())\n",
    "    vocab_negsample_rate = {key: vocab_negsample_rate[key] / negsample_rate_sum for key in vocab_counts.keys()}\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        for i in range(len(document)):\n",
    "            target = document[i]\n",
    "            target_index = vocab_to_id[target]\n",
    "\n",
    "            context = []\n",
    "            for j in range(max(0, i - left_context_window), min(i + right_context_window + 1, len(document))):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context.append(vocab_to_id(document[j]))\n",
    "                \n",
    "            targets.append(target_index)\n",
    "            contexts.append(context)\n",
    "            labels.append(1)\n",
    "\n",
    "            for neg_target_index in np.random.choice(range(len(vocab_negsample_rate)), \n",
    "                                                     size=k_negsample, replace=False, \n",
    "                                                     p=list(vocab_negsample_rate.values())):\n",
    "                targets.append(neg_target_index)\n",
    "                contexts.append(context)\n",
    "                labels.append(0)\n",
    "                    \n",
    "    return (jnp.array(contexts), jnp.array(targets), jnp.array(labels))\n",
    "    \n",
    "    vocab = list(vocab_counts.keys())\n",
    "    counts = list(vocab_counts.values())\n",
    "    total_counts = float(sum(counts))\n",
    "    vocab_subsample_rate = {key: subsample_prob(vocab_counts[key] / total_counts, subsample_rate) for key in vocab_counts.keys()}\n",
    "    vocab_negsample_rate = {key: math.pow(vocab_counts[key], 3/4) for key in vocab_counts.keys()}\n",
    "    negsample_rate_sum = sum(vocab_negsample_rate.values())\n",
    "    vocab_negsample_rate = {key: vocab_negsample_rate[key] / negsample_rate_sum for key in vocab_counts.keys()}\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    contexts = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        for i in range(len(document)):\n",
    "            target = document[i]\n",
    "\n",
    "            context_window = [document[j] for j in range(max(0, i - left_context_window), i)] + [document[j] for j in range(i+1, min(i + right_context_window + 1, len(document)))]\n",
    "            \n",
    "            U = np.random.uniform(0,1)\n",
    "            if U < vocab_subsample_rate[target]:\n",
    "                continue\n",
    "\n",
    "            context = []\n",
    "            for context_word in context_window:\n",
    "                V = np.random.uniform(0,1)\n",
    "                if V < vocab_subsample_rate[context_word]:\n",
    "                    context.append(vocab.index(context_word))\n",
    "            while len(context) < left_context_window + right_context_window:\n",
    "                context.append(-1)\n",
    "            \n",
    "            targets.append(vocab.index(target))\n",
    "            contexts.append(context)\n",
    "            labels.append(1)\n",
    "\n",
    "            if k_negsample > 0:\n",
    "                neg_target_words = np.random.choice(vocab, size=k_negsample, replace=False, p=list(vocab_negsample_rate.values()))\n",
    "                for neg_target_word in neg_target_words:\n",
    "                    targets.append(vocab.index(neg_target_word))\n",
    "                    contexts.append(context)\n",
    "                    labels.append(0)\n",
    "                    \n",
    "    return (jnp.array(contexts), jnp.array(targets), jnp.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba82eb1-d369-4e29-860d-54ed7dafcc66",
   "metadata": {},
   "source": [
    "## Softmax Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1803c7f6-b916-4de1-9b54-9c98cf111e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / jnp.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "103f6239-0487-488b-aba6-046c4499a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def softmax_skipgram_net(params, target):\n",
    "    output = params['E'][target]\n",
    "    output = output @ params['Theta']\n",
    "    output = softmax(output)\n",
    "    return output\n",
    "\n",
    "@jax.jit\n",
    "def softmax_skipgram_loss(params, target, y_true):\n",
    "    y_pred = softmax_skipgram_net(params, target)\n",
    "    return jnp.mean(-jnp.sum(jnp.log(y_pred + 1e-8) * y_true, axis=1))\n",
    "softmax_skipgram_loss_value_and_grad = jax.jit(jax.value_and_grad(softmax_skipgram_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d43e6b09-2a3f-4bfb-aa43-dc234ba937d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def softmax_cbow_net(params, context):\n",
    "    output = np.mean(params['E'][context], axis=1)\n",
    "    output = output @ params['Theta']\n",
    "    output = softmax(output)\n",
    "    return output\n",
    "\n",
    "@jax.jit\n",
    "def softmax_cbow_loss(params, context, y_true):\n",
    "    y_pred = softmax_cbow_net(params, context)\n",
    "    return jnp.mean(-jnp.sum(jnp.log(y_pred + 1e-8) * y_true, axis=1))\n",
    "softmax_cbow_loss_value_and_grad = jax.jit(jax.value_and_grad(softmax_cbow_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f2e04-6ad9-4e21-a05d-9b3e9bf317f1",
   "metadata": {},
   "source": [
    "## Negative Sampling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71130ed0-c751-41cb-a192-72a871ec1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + jnp.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8d9e3-2c2d-4ee4-8b32-4b5362d0397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def negsample_skipgram_net(params, context, target):\n",
    "    target_embedding = params['E'][target]\n",
    "    context_parameter = params['Theta'][context]\n",
    "    output = jnp.sum(target_embedding * context_parameter, axis=1)\n",
    "    output = sigmoid(output)\n",
    "    return output\n",
    "\n",
    "@jax.jit\n",
    "def negsample_skipgram_loss(params, context, target, label_true):\n",
    "    label_pred = negsample_skipgram_net(params, context, target)\n",
    "    return jnp.mean(-jnp.log(jnp.where(label_true == 1, label_pred, 1-label_pred)))\n",
    "negsample_skipgram_loss_value_and_grad = jax.jit(jax.value_and_grad(negsample_skipgram_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e48817-5fb9-4f49-b7d8-ec3cb52b1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def negsample_cbow_net(params, context, target):\n",
    "    context_embedding = jnp.mean(params['E'][context], axis=1)\n",
    "    target_parameter = params['Theta'][target]\n",
    "    output = jnp.sum(context_embedding * target_parameter, axis=1)\n",
    "    output = sigmoid(output)\n",
    "    return output\n",
    "\n",
    "@jax.jit\n",
    "def negsample_cbow_loss(params, context, target, label_true):\n",
    "    label_pred = negsample_cbow_net(params, context, target)\n",
    "    return jnp.mean(-jnp.log(jnp.where(label_true == 1, label_pred, 1-label_pred)))\n",
    "negsample_cbow_loss_value_and_grad = jax.jit(jax.value_and_grad(negsample_cbow_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bf6bf-132e-40e1-9473-13c22429795d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852129b4-cadb-4083-b331-f8ad323ad05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_negsample = 0 to do softmax else negative sampling\n",
    "class Model:\n",
    "    def __init__(self, mode, vocab_counts, n_embeddings, left_context_window, right_context_window, k_negsample):\n",
    "        assert mode in [\"skipgram\", \"cbow\"]\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.vocab_counts = vocab_counts\n",
    "        self.vocab_size = len(vocab_counts)\n",
    "        self.vocab_to_id = dict(zip(vocab_counts.keys(), range(len(mario_vocab))))\n",
    "        self.id_to_vocab = dict(zip(range(len(mario_vocab)), vocab_counts.keys()))\n",
    "        \n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.left_context_window = left_context_window\n",
    "        self.right_context_window = right_context_window\n",
    "        self.k_negsample = k_negsample\n",
    "        \n",
    "        self.initialize_params()\n",
    "\n",
    "    def initialize_params(self):\n",
    "        np.random.seed(seed)\n",
    "        if self.k_negsample > 0:\n",
    "            self.params = {\n",
    "                'E': np.random.normal(0, np.sqrt(1/self.vocab_size), (self.vocab_size, self.n_embeddings)),\n",
    "                'Theta': np.random.normal(0, np.sqrt(1/self.vocab_size), (self.vocab_size, self.n_embeddings))\n",
    "            }\n",
    "        else:\n",
    "            self.params = {\n",
    "                'E': np.random.normal(0, np.sqrt(1/self.vocab_size), (self.vocab_size, self.n_embeddings)),\n",
    "                'Theta': np.random.normal(0, np.sqrt(1/self.n_embeddings), (self.n_embeddings, self.vocab_size))\n",
    "            }\n",
    "\n",
    "    def train(self, documents, \n",
    "              lr, beta1, beta2, epsilon, \n",
    "              n_epochs, batch_size, subsample_rate):\n",
    "        \n",
    "        documents_subsampled = subsample(documents)\n",
    "        \n",
    "        if self.mode == \"skipgram\":\n",
    "            contexts, targets, labels = skipgram(documents_subsampled, self.vocab_counts, self.left_context_window, self.right_context_window, self.k_negsample)\n",
    "        elif self.mode == \"cbow\":\n",
    "            contexts, targets, labels = cbow(documents_subsampled, self.vocab_counts, self.left_context_window, self.right_context_window, self.k_negsample)\n",
    "        \n",
    "        n_batches = int(len(contexts) / batch_size)\n",
    "        self.loss_history = []\n",
    "        self.time_history = []\n",
    "        start_time = time.time()\n",
    "        m = None\n",
    "        v = None\n",
    "        np.random.seed(seed)\n",
    "        for epoch in range(n_epochs):\n",
    "            shuffled_indices = np.random.permutation(len(contexts))\n",
    "            shuffled_contexts = contexts[shuffled_indices]\n",
    "            shuffled_targets = targets[shuffled_indices]\n",
    "            shuffled_labels = labels[shuffled_indices]\n",
    "            \n",
    "            epoch_loss_history = []\n",
    "            for batch in range(n_batches):\n",
    "                context_batch = shuffled_contexts[batch*batch_size : (batch+1)*batch_size]\n",
    "                target_batch = shuffled_targets[batch*batch_size : (batch+1)*batch_size]\n",
    "                label_batch = shuffled_labels[batch*batch_size : (batch+1)*batch_size]\n",
    "\n",
    "                if self.k_negsample > 0:\n",
    "                    if self.mode == \"skipgram\":\n",
    "                        loss, loss_grad = negsample_skipgram_loss_value_and_grad(self.params, context_batch, target_batch, label_batch)\n",
    "                    elif self.mode == \"cbow\":\n",
    "                        loss, loss_grad = negsample_cbow_loss_value_and_grad(self.params, context_batch, target_batch, label_batch)\n",
    "                else:\n",
    "                    if self.mode == \"skipgram\":\n",
    "                        one_hot_context_batch = jnp.array(batch_one_hot(context_batch, self.vocab, True))\n",
    "                        loss, loss_grad = softmax_skipgram_loss_value_and_grad(self.params, target_batch, one_hot_context_batch)\n",
    "                    elif self.mode == \"cbow\":\n",
    "                        one_hot_target_batch = jnp.array(batch_one_hot(target_batch, self.vocab, True))\n",
    "                        loss, loss_grad = softmax_cbow_loss_value_and_grad(self.params, context_batch, one_hot_target_batch)\n",
    "                    \n",
    "                if m == None:\n",
    "                    m = loss_grad\n",
    "                    v = {key: loss_grad[key]**2 for key in loss_grad.keys()}\n",
    "                else:\n",
    "                    m = {key: beta1*m[key] + (1-beta1)*loss_grad[key] for key in loss_grad.keys()}\n",
    "                    v = {key: beta2*v[key] + (1-beta2)*(loss_grad[key]**2) for key in loss_grad.keys()}\n",
    "                    \n",
    "                self.params = {key: self.params[key] - lr * m[key] / (epsilon + jnp.sqrt(v[key])) for key in self.params.keys()}\n",
    "                \n",
    "                epoch_loss_history.append(loss)\n",
    "        \n",
    "            self.loss_history.append(np.mean(epoch_loss_history))\n",
    "            self.time_history.append(time.time() - start_time)\n",
    "\n",
    "    # takes in the word for skipgram, list of words for cbow\n",
    "    def predict(self, X):\n",
    "        if self.k_negsample > 0:\n",
    "            if self.mode == \"skipgram\":\n",
    "                return negsample_skipgram_net(self.params, jnp.array([self.vocab_to_id(X)]))\n",
    "            elif self.mode == \"cbow\":\n",
    "                return negsample_cbow_net(self.params, jnp.array([[self.vocab_to_id(context) for context in X]]))\n",
    "        else:\n",
    "            if self.mode == \"skipgram\":\n",
    "                return softmax_skipgram_net(self.params, jnp.array([self.vocab_to_id(X)]))\n",
    "            elif self.mode == \"cbow\":\n",
    "                return softmax_cbow_net(self.params, jnp.array([[self.vocab_to_id(context) for context in X]]))\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        return one_hot(word, self.vocab) @ self.params['E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d99f07-6f7b-45bf-b81e-eae64ddc5b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mario_model_skipgram2 = Model(mario_vocab_counts, 300, 2, 2, \"skipgram\", 10)\n",
    "mario_model_skipgram5 = Model(mario_vocab_counts, 300, 5, 5, \"skipgram\", 10)\n",
    "mario_model_cbow2 = Model(mario_vocab_counts, 100, 2, 2, \"cbow\", 10)\n",
    "mario_model_cbow5 = Model(mario_vocab_counts, 100, 5, 5, \"cbow\", 10)\n",
    "\n",
    "mario_model_skipgram2.train(mario_documents_sentences, 0.01, 0.9, 0.999, 1e-8, 10000, 100, 0.0001)\n",
    "mario_model_skipgram5.train(mario_documents_sentences, 0.01, 0.9, 0.999, 1e-8, 10000, 100, 0.0001)\n",
    "mario_model_cbow2.train(mario_documents_sentences, 0.01, 0.9, 0.999, 1e-8, 10000, 100, 0.0001)\n",
    "mario_model_cbow5.train(mario_documents_sentences, 0.01, 0.9, 0.999, 1e-8, 10000, 100, 0.0001)\n",
    "\n",
    "model_path = \"models/\"\n",
    "with open(os.path.join(model_path, 'mario_model_skipgram2.pkl'), 'wb') as file:\n",
    "    pkl.dump(mario_model_skipgram2, file)\n",
    "with open(os.path.join(model_path, 'mario_model_skipgram5.pkl'), 'wb') as file:\n",
    "    pkl.dump(mario_model_skipgram5, file)\n",
    "with open(os.path.join(model_path, 'mario_model_cbow2.pkl'), 'wb') as file:\n",
    "    pkl.dump(mario_model_cbow2, file)\n",
    "with open(os.path.join(model_path, 'mario_model_cbow5.pkl'), 'wb') as file:\n",
    "    pkl.dump(mario_model_cbow5, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a49d9-10c5-4374-aa53-1b3b077a780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mario_model_skipgram2.time_history, mario_model_skipgram2.loss_history)\n",
    "plt.plot(mario_model_skipgram5.time_history, mario_model_skipgram5.loss_history)\n",
    "plt.plot(mario_model_cbow2.time_history, mario_model_cbow2.loss_history)\n",
    "plt.plot(mario_model_cbow5.time_history, mario_model_cbow5.loss_history)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.legend([\"skipgram2\", \"skipgram5\", \"cbow2\", \"cbow5\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d6647-7c57-4c39-973c-61016414b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mario_model_skipgram2.predict(\"princess\")\n",
    "sorted_indices = jnp.flip(jnp.argsort(p[0]))\n",
    "np.array(list(zip(mario_vocab, p[0].tolist())))[sorted_indices][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b51aaf-487c-41dc-bcfc-dabf43dc4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cosine_sim(a, b):\n",
    "    return jnp.dot(a, b) / (jnp.linalg.norm(a) * jnp.linalg.norm(b))\n",
    "\n",
    "batch_cosine_sim = jax.jit(jax.vmap(cosine_sim, (None, 0), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094efb69-142f-42d8-b0c0-afc9cbf83d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mario_model_skipgram2\n",
    "\n",
    "m = model.get_embedding(\"mario\")\n",
    "sim = batch_cosine_sim(m, model.params['E'])\n",
    "sorted_indices = jnp.flip(jnp.argsort(sim))\n",
    "np.array(list(zip(mario_vocab, sim.tolist())))[sorted_indices][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
